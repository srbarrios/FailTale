# Configuration template for the Server
# Copy this file to config.yaml and edit it.
# DO NOT commit config.yaml if it contains secrets!

# Global SSH configuration (can be overridden per host if necessary)
ssh_defaults:
  username: "your_limited_ssh_user"  # User with minimal permissions on target hosts
  private_key_path: "~/.ssh/id_rsa"  # Path to your SSH private key
  port: 22
  connection_timeout: 10 # Seconds
  command_timeout: 30    # Seconds

# Definition of what data to collect per component role
components:
  server:
    useful_data:
      - description: "Last 100 lines of the server's main log"
        command: "tail -n 100 /var/log/myapp/server.log"
      - description: "Status of the main service"
        command: "systemctl status myapp.service || echo 'systemctl not available'" # Add fallback
      - description: "Processes related to myapp"
        command: "ps aux | grep '[m]yapp' || echo 'grep found no processes'"
      - description: "Listening ports (TCP)"
        command: "ss -tlpn | grep LISTEN || netstat -tlpn | grep LISTEN || echo 'ss/netstat command not available'"
  minion:
    useful_data:
      - description: "Last 50 lines of the minion log"
        command: "tail -n 50 /var/log/myapp/minion.log"
      - description: "Disk usage (root partition)"
        command: "df -h /"
  proxy:
     useful_data:
       - description: "Last 50 lines of the proxy log (nginx example)"
         command: "tail -n 50 /var/log/nginx/access.log"
       - description: "Recent proxy errors (nginx example)"
         command: "tail -n 50 /var/log/nginx/error.log"

# (Optional) Definition of specific hosts if not passed dynamically
# hosts:
#   - hostname: "server1.example.com"
#     role: "server"
#     mandatory: True
#     # Override defaults if necessary:
#     # ssh_username: "another_user"
#   - hostname: "192.168.1.110"
#     role: "minion"

# LLMs configurations
default_llm_provider: "gemini" # Default LLM to use for analysis
base_prompt: "You are an expert in Uyuni project." # Prompt to be prepended in LLMs calls

ollama:
  base_url: "http://localhost:11434"            # Base URL of your Ollama instance
  model: "mistral"                              # Model to use for analysis
  embedding_model: "nomic-embed-text"           # Model to use for embeddings
  request_timeout: 180                          # Timeout for requests to Ollama
  persist_directory: "examples/uyuni/chroma_db" # Chroma DB path
  seed: 42                                      # Seed to generate the same text for the same prompt.
  multimodal: true                              # Whether the model supports multimodal input

gemini:
  # api_key: "YOUR_GEMINI_KEY_DIRECTLY_HERE" # Direct key
  api_key_env_var: "GEMINI_API_KEY"        # Custom Env Var
  model: "gemini-2.5-flash"                # Model to use for analysis
  multimodal: true                         # Whether the model supports multimodal input

openai:
  # api_key: "YOUR_OPENAI_KEY_DIRECTLY_HERE" # Direct key
  api_key_env_var: "OPENAI_API_KEY"        # Custom Env Var
  model: "gpt-4.1"                         # Model to use for analysis
  multimodal: true                         # Whether the model supports multimodal input
